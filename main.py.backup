# main.py (CSS 제거 및 파싱 로직 개선)

import time
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
from bs4 import BeautifulSoup
import re
import argparse

def fetch_html_with_selenium(url: str) -> str | None:
    """Selenium을 사용하여 브라우저를 자동화하고, 최종 렌더링된 HTML을 가져옵니다."""
    print(f"[{url}] Selenium으로 접속을 시도합니다...")
    
    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36")

    driver = None
    try:
        service = Service()
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60) 
        driver.get(url)
        
        wait = WebDriverWait(driver, 20)
        wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, "h1.page-title, div.content")))
        
        html_content = driver.page_source
        print(f"[{url}] HTML 가져오기 성공!")
        return html_content

    except Exception as e:
        print(f"[{url}] 오류: Selenium으로 페이지를 가져오는 데 실패했습니다 - {e}")
        return None
    finally:
        if driver:
            driver.quit()

def parse_adobe_content_blocks(selected_blocks: list) -> str:
    """
    선택된 콘텐츠 블록 리스트(BeautifulSoup 요소들)를 받아
    h2, h3, p, ul, ol 태그를 기반으로 마크다운 형식으로 구조화합니다.
    """
    output_lines = []
    if not selected_blocks:
        return ""

    for block in selected_blocks:
        # H2 (섹션 제목) 처리
        h2_tag = block.find('h2', class_='section-title', recursive=False)
        if not h2_tag:
            cmp_text_for_h2 = block.find('div', class_='cmp-text')
            if cmp_text_for_h2:
                h2_tag = cmp_text_for_h2.find('h2', class_='section-title', recursive=False)
                if not h2_tag:
                    h2_tag = cmp_text_for_h2.find('h2', recursive=False)
        if h2_tag:
            text = h2_tag.get_text(strip=True)
            if text:
                output_lines.append(f"\n## {text}\n")

        # H3 및 변수 제목 처리
        # span.help-variable-title을 먼저 찾고, 일반 h3도 찾습니다.
        # 다만, li.step 내부에 있는 span.help-variable-title은 li의 텍스트로 통합되므로 여기서는 중복 출력하지 않습니다.
        var_titles = block.select('div.variable > p.text > span.help-variable-title')
        for var_title_span in var_titles:
            if not var_title_span.find_parent('li', class_='step'): # li.step의 자식이 아닌 경우에만 H3로 만듦
                text = var_title_span.get_text(strip=True)
                if text:
                    # 이미 H2 바로 다음에 오는 경우가 많으므로, 불필요한 추가 줄바꿈은 피함
                    if output_lines and output_lines[-1].strip().startswith("##"):
                         output_lines.append(f"### {text}")
                    else:
                         output_lines.append(f"\n### {text}")


        h3_tags_in_block = block.find_all('h3', recursive=False)
        if not h3_tags_in_block:
             cmp_text_for_h3 = block.find('div', class_='cmp-text')
             if cmp_text_for_h3:
                 h3_tags_in_block = cmp_text_for_h3.find_all('h3', recursive=False)
        for h3_tag_item in h3_tags_in_block:
            text = h3_tag_item.get_text(strip=True)
            if text:
                if output_lines and output_lines[-1].strip().startswith("##"):
                    output_lines.append(f"### {text}\n")
                else:
                    output_lines.append(f"\n### {text}\n")


        # P (문단) 처리
        cmp_text_divs = block.find_all('div', class_='cmp-text')
        if not cmp_text_divs and ('text' in block.get('class', []) and not block.find('h2', class_='section-title')):
             cmp_text_divs = [block]

        for cmp_text_div in cmp_text_divs:
            paragraphs = cmp_text_div.find_all('p', recursive=False)
            for p_tag in paragraphs:
                if p_tag.find('span', class_='help-variable-title'):
                    continue 
                
                # 이미지가 포함된 p태그인지 확인 (예: <p><img ...></p>) - 현재는 텍스트만 추출
                if p_tag.find('img'):
                    # 이미지 캡션 등을 추출하고 싶다면 여기에 로직 추가 가능
                    # 지금은 이미지 포함 p태그는 건너뛰거나, 텍스트만 가져오도록 할 수 있습니다.
                    # 여기서는 일단 get_text()가 텍스트만 가져오도록 둡니다.
                    pass

                text = p_tag.get_text(strip=True)
                if text:
                    if output_lines and not output_lines[-1].strip().startswith(("#", ">", "*", "1.", "2.", "3.")): # 연속된 p 태그 사이에 공백
                         if output_lines[-1].strip() != "": # 이전 줄이 비어있지 않으면
                            output_lines.append("") 
                    output_lines.append(text)
        
        # UL/OL (목록) 처리
        lists = block.find_all(['ul', 'ol']) # 블록 내 모든 레벨의 리스트를 찾음
        for list_tag in lists:
            # 이 리스트가 이미 다른 리스트의 li 항목 내부에서 처리되지 않았는지 확인 (중복 방지)
            # 예를 들어, li > p > ul 구조일때, li.get_text()로 이미 처리됨
            if list_tag.find_parent('li'): 
                # 단, 최상위 리스트의 직계 자손 li가 아닌, li 내부의 중첩된 리스트는 여기서 처리해야 할 수 있음.
                # 지금은 단순화를 위해, li 안에 중첩된 ul/ol은 li.get_text()에 의해 텍스트로 포함된다고 가정.
                # 더 정교한 처리가 필요하면, li 내부를 재귀적으로 파싱하는 로직 추가.
                # 여기서는 li.get_text(strip=True)가 잘 처리해줄 것으로 기대하고 넘어갑니다.
                pass

            list_items_text = []
            for i, li in enumerate(list_tag.find_all('li', recursive=False)): # 현재 list_tag의 직접적인 자식 li만
                item_text = li.get_text(strip=True) # li 내부의 모든 텍스트를 합침
                if item_text:
                    prefix = '* ' if list_tag.name == 'ul' else f"{i + 1}. "
                    list_items_text.append(f"{prefix}{item_text}")
            
            if list_items_text:
                if output_lines and output_lines[-1].strip() != "" and not output_lines[-1].strip().endswith("\n"):
                    output_lines.append("")
                output_lines.extend(list_items_text)
                output_lines.append("")

        # 참고/주의사항
        note_divs = block.find_all('div', class_=['helpx-note', 'helpx-caution'])
        for note_div in note_divs:
            note_title_span = note_div.find('span', class_='note-title')
            title_text = note_title_span.get_text(strip=True) if note_title_span else ("참고:" if "helpx-note" in note_div.get('class',[]) else "주의:")
            
            note_body_text = ""
            note_body_cmp = note_div.find('div', class_='cmp-text')
            if note_body_cmp:
                # cmp-text 내부의 p 태그들을 찾아 각각의 텍스트를 결합
                p_tags_in_note = note_body_cmp.find_all('p')
                note_body_text = "\n".join([p.get_text(strip=True) for p in p_tags_in_note if p.get_text(strip=True)])
            else:
                p_tag_in_note = note_div.find('p')
                note_body_text = p_tag_in_note.get_text(strip=True) if p_tag_in_note else ""

            if note_body_text:
                if output_lines and output_lines[-1].strip() != "":
                     output_lines.append("")
                output_lines.append(f"> **{title_text}**")
                # 인용구의 각 줄을 > 로 시작하도록 처리
                for line in note_body_text.splitlines():
                    if line.strip():
                        output_lines.append(f"> {line.strip()}")
                output_lines.append("")
                
    final_output = []
    if output_lines:
        was_empty_line = True # 첫 줄 앞에 불필요한 공백 방지
        for line in output_lines:
            current_line_empty = (line.strip() == "")
            if current_line_empty and was_empty_line:
                continue
            final_output.append(line)
            was_empty_line = current_line_empty
            
    return "\n".join(final_output).strip()


def save_to_markdown(title: str, content: str):
    safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip().replace(" ", "_")
    filename = f"{safe_title}.md"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"성공: '{filename}' (마크다운) 파일이 저장되었습니다.")
    except Exception as e:
        print(f"오류: '{filename}' (마크다운) 파일 저장에 실패했습니다 - {e}")

def save_raw_html(title: str, html_content: str):
    safe_title = re.sub(r'[\\/*?:"<>|]', "", title).strip().replace(" ", "_")
    filename = f"원본HTML_{safe_title}.html"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(html_content)
        print(f"성공: '{filename}' (원본 HTML) 파일이 저장되었습니다.")
    except Exception as e:
        print(f"오류: '{filename}' (원본 HTML) 파일 저장에 실패했습니다 - {e}")

# --- 프로그램 실행 부분 ---
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Selenium을 사용하여 웹 페이지 URL을 마크다운 문서로 변환하고, 원본 HTML도 저장합니다.")
    parser.add_argument('-u', '--urls', nargs='+', required=True, help="변환할 하나 이상의 URL 목록 (공백으로 구분)")
    args = parser.parse_args()

    EXPORT_DISK_CD_URL_IDENTIFIER = "export-files-disk-or-cd.html"
    SPECIFIC_CONTENT_START_PHRASE = "사진을 하드 디스크, CD 또는 DVD로 내보내려면, 다음 절차를 따르십시오."
    SPECIFIC_CONTENT_END_PHRASE = "Export Actions 만들기를 참조하십시오."

    for url in args.urls:
        print("-" * 50)
        print(f"처리 중인 URL: {url}")
        html_content = fetch_html_with_selenium(url)
        
        if not html_content:
            print(f"[{url}] HTML 내용을 가져오지 못했습니다. 다음 URL로 넘어갑니다.")
            continue

        soup = BeautifulSoup(html_content, 'lxml')

        # --- CSS 및 스크립트 태그 제거 ---
        for unwanted_tag in soup.find_all(['style', 'script']):
            unwanted_tag.decompose()
        # --- 제거 끝 ---

        page_overall_title_tag = soup.find('title')
        page_overall_title = page_overall_title_tag.get_text(strip=True) if page_overall_title_tag else f"제목_없음_{int(time.time())}"

        save_raw_html(page_overall_title, soup.prettify()) # 정돈된 HTML 저장

        main_content_column = soup.select_one('div.dexter-FlexContainer-Items > div.position')
        
        actual_article_blocks_container = None
        if main_content_column:
            actual_article_blocks_container = main_content_column.find('div', class_=lambda x: x and x.startswith('aem-Grid aem-Grid--12'))
        
        if not actual_article_blocks_container: 
             actual_article_blocks_container = main_content_column if main_content_column else soup.find('main')

        if actual_article_blocks_container:
            h1_tag = actual_article_blocks_container.find('h1', class_='page-title') 
            if not h1_tag: 
                 h1_tag = soup.find('h1', class_='page-title')
            doc_main_title = h1_tag.get_text(strip=True) if h1_tag else page_overall_title
            
            # 실제 콘텐츠 블록들을 찾습니다.
            # titleBar는 H1을 포함하므로, 그 이후 블록부터 실제 내용으로 간주할 수 있습니다.
            # 또는 titleBar를 포함하여 h1도 parse_adobe_content_blocks에서 처리하도록 할 수도 있습니다.
            # 여기서는 titleBar 이후의 주요 블록들을 대상으로 합니다.
            content_block_candidates = actual_article_blocks_container.find_all(
                lambda tag: tag.name == 'div' and \
                            any(cls in tag.get('class', []) for cls in ['text', 'procedure', 'imagepar', 'internalBanner', 'reference']), \
                recursive=False 
            )
            if not content_block_candidates: # 직계 자식으로 못찾으면 좀 더 깊이 탐색
                 content_block_candidates = actual_article_blocks_container.find_all(
                    lambda tag: tag.name == 'div' and \
                                any(cls in tag.get('class', []) for cls in ['text', 'procedure', 'imagepar'])
                )
            
            blocks_to_parse = []
            if EXPORT_DISK_CD_URL_IDENTIFIER in url:
                print(f"'{url}'에 대해 특정 내용 추출을 시도합니다.")
                collecting = False
                for block in content_block_candidates: # 여기서 content_block_candidates 사용
                    block_text_for_phrase_check = block.get_text(strip=True)
                    if not collecting and SPECIFIC_CONTENT_START_PHRASE in block_text_for_phrase_check:
                        collecting = True
                    if collecting:
                        blocks_to_parse.append(block)
                    if collecting and SPECIFIC_CONTENT_END_PHRASE in block_text_for_phrase_check:
                        break 
                if not blocks_to_parse:
                    print(f"경고: '{url}'에서 특정 내용 시작/끝 부분을 찾지 못했습니다. 전체 주요 콘텐츠를 파싱합니다.")
                    blocks_to_parse = content_block_candidates
            else:
                print(f"'{url}'에 대해 전체 주요 콘텐츠 영역 파싱을 시도합니다.")
                blocks_to_parse = content_block_candidates
            
            structured_text = parse_adobe_content_blocks(blocks_to_parse)
            
            final_md_content = f"# {doc_main_title}\n\n{structured_text}"
            save_to_markdown(doc_main_title, final_md_content)
        else:
            print(f"[{url}]에서 주요 콘텐츠 영역을 찾지 못했습니다.")
            print(f"'{page_overall_title}.md' 파일은 생성되지 않았지만, 원본 HTML 파일은 저장되었습니다.")